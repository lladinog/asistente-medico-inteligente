# Configuración del frontend
DEBUG=true
HOST=127.0.0.1
PORT=8000

# Path al modelo GGUF a utilizar
MODEL_PATH=C:/ruta/a/tu/modelo.gguf

# --- Configuración de LlamaCpp ---
# Número de hilos de CPU a utilizar. Por defecto, usa todos los disponibles.
LLAMA_N_THREADS=4

# Número de tokens a procesar en paralelo.
LLAMA_N_BATCH=256

# Tamaño del contexto del modelo.
LLAMA_N_CTX=2048 